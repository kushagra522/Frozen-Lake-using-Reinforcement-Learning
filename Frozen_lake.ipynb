{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](lake.jpg \"Lake\")\n",
    "\n",
    "Source: [DeepLizard](https://deeplizard.com/images/frozen%20lake%20winter.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|STATE    |DESCRIPTION                      |REWARD    |\n",
    "|:--|:--|--:|\n",
    "| S       |Agentâ€™s starting point - safe    |   0      |\n",
    "| F       |Frozen surface - safe            |   0      |\n",
    "| H       |Hole - game over                 |   0      |\n",
    "| G       |Goal - game over                 |   1      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Initializing the parameters on enviroment\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size,action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the environment parameters\n",
    "\n",
    "num_episodes = 10000               #The no. epsiode we want our agent to play during training.\n",
    "max_steps_per_episode = 100       #Max no. of steps agent allowed to take during single episode.\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "\n",
    "#Parameters related to exploitation greedy-strategy.\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.0001\n",
    "exploration_decay_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.05300000000000004\n",
      "2000 :  0.18400000000000014\n",
      "3000 :  0.4100000000000003\n",
      "4000 :  0.5920000000000004\n",
      "5000 :  0.6890000000000005\n",
      "6000 :  0.7150000000000005\n",
      "7000 :  0.7260000000000005\n",
      "8000 :  0.7330000000000005\n",
      "9000 :  0.7490000000000006\n",
      "10000 :  0.7560000000000006\n",
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.53590214 0.48665914 0.49401431 0.48728632]\n",
      " [0.39186371 0.3739457  0.33973766 0.52584175]\n",
      " [0.40221006 0.39976444 0.39049296 0.51445095]\n",
      " [0.28942028 0.24499417 0.36213343 0.50147623]\n",
      " [0.55623894 0.30590888 0.36450878 0.39260613]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30880851 0.13282312 0.15398423 0.12008024]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44528971 0.45443821 0.41596886 0.57067387]\n",
      " [0.50403974 0.58037094 0.42658771 0.42684557]\n",
      " [0.56635877 0.37272724 0.34628043 0.26836233]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47980086 0.60171351 0.77733383 0.4896073 ]\n",
      " [0.73218955 0.88610972 0.72873692 0.72785792]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "#Q-learning Algorithm\n",
    "for episode in range(num_episodes):\n",
    " \n",
    "    \"\"\"\n",
    "    For each episode we are going\n",
    "    back to starting state.\n",
    "    \"\"\"\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False                     #Just to keep track that whether our episode is finished or not.\n",
    "    rewards_current_episode = 0      #Reset reward\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps_per_episodes):\n",
    "         \n",
    "        \"\"\"\n",
    "        Nested loop which run for each timestamp\n",
    "        within an episode.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Exploration-rate trade off:\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state , reward , done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table for (s,a)\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    #Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "# Calculate and print the average reward per thousand reward\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        state = new_state    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6-TF2.3]",
   "language": "python",
   "name": "conda-env-py3.6-TF2.3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
